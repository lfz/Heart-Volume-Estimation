{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 3007)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import InputLayer, Conv2DLayer, MaxPool2DLayer, DenseLayer, GlobalPoolLayer, Upscale2DLayer\n",
    "from lasagne.layers import ElemwiseSumLayer, NonlinearityLayer, SliceLayer, ConcatLayer, ScaleLayer\n",
    "from lasagne.layers import dropout, batch_norm\n",
    "from lasagne.nonlinearities import rectify, softmax, sigmoid\n",
    "from lasagne.init import GlorotNormal, GlorotUniform, HeUniform, HeNormal\n",
    "from lasagne.objectives import squared_error, categorical_crossentropy, categorical_accuracy, binary_accuracy\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import random\n",
    "import theano\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "import benchmark as bm\n",
    "\n",
    "from fcn1.adapter import adapter as adapter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_volume(area, location, resolution):\n",
    "    def _step(idx_, prior_result_, area_, location_, resolution_):\n",
    "        area1 = area_[idx_][0] * T.prod(resolution_[idx_])\n",
    "        area2 = area_[idx_ + 1][0] * T.prod(resolution_[idx_ + 1])\n",
    "        h = location_[idx_ + 1] - location_[idx_]\n",
    "        volume = (area1 + area2 )* np.prod(fixed_size).astype('float32') * h / 2.0 / 1000\n",
    "        return prior_result_ + volume\n",
    "\n",
    "    predict_V_list, _ = theano.scan(fn=_step,\n",
    "                              outputs_info = np.array([0.]).astype('float32'),\n",
    "                              sequences = T.arange(1000),\n",
    "                              non_sequences = [area, location, resolution],\n",
    "                              n_steps = location.shape[0] - 1)\n",
    "    predict_V = predict_V_list[-1]\n",
    "    return predict_V[0]\n",
    "\n",
    "def stage3_load_batch_records(file_path, fixed_size):\n",
    "    data = np.load(file_path).item()\n",
    "    patch_list = data['patchStack']\n",
    "    location_list = np.array(data['SliceLocation'])\n",
    "    resolution = np.array(data['PixelSpacing'])\n",
    "    resized_resolution_list = []\n",
    "    resized_patch_list = []\n",
    "    for patch in patch_list:\n",
    "        resized_resolution_list.append(\n",
    "            (resolution[0] / fixed_size[0] * patch.shape[0], resolution[1] / fixed_size[1] * patch.shape[1]))\n",
    "        resized_patch_list.append(cv2.resize(patch, fixed_size))\n",
    "\n",
    "    resized_patch_list = np.array(resized_patch_list, dtype='float32')[:, None, :, :]\n",
    "    location_list = np.array(location_list, dtype='float32')\n",
    "    resized_resolution_list = np.array(resized_resolution_list, dtype='float32')\n",
    "    \n",
    "    patch_batch = [resized_patch_list, resized_patch_list[:,:,::-1,:],\n",
    "                      resized_patch_list[:,:,:,::-1], resized_patch_list[:,:,::-1,::-1]]\n",
    "    location_batch = [location_list] * 4\n",
    "    resolution_batch = [resized_resolution_list] * 4\n",
    "    return patch_batch, location_batch, resolution_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "snapshot_root = 'snapshot'\n",
    "snapshot_file = '0.npz'\n",
    "snapshot_iter = 1\n",
    "schedules = [2] * 3005\n",
    "fixed_size = (48, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------data info---------------------\n",
      "TRAIN, x: 3200, location: 3200, resolution: 3200, volume: 3200\n",
      "VAL, x: 800, location: 800, resolution: 800, volume: 800\n"
     ]
    }
   ],
   "source": [
    "train_val_ratio = 0.8\n",
    "\n",
    "# read train and val volume data\n",
    "volume_csv_path = '../../clean/stage3/train.csv'\n",
    "volume_csv = pd.read_csv(volume_csv_path)\n",
    "volume_data = np.array(volume_csv.iloc[:, 1:3])\n",
    "rows = volume_data.shape[0]\n",
    "volume_data = np.repeat(volume_data.flatten(), 4).astype('float32')\n",
    "\n",
    "# read train and val patch, location and resolution\n",
    "root_dir = '../../clean/stage3'\n",
    "min_root_dir = os.path.join(root_dir, 'min')\n",
    "max_root_dir = os.path.join(root_dir, 'max')\n",
    "x_data = []\n",
    "location_data = []\n",
    "resolution_data = []\n",
    "for i in range(1, rows + 1):\n",
    "    min_full_path = os.path.join(min_root_dir, str(i) + '.npy')\n",
    "    max_full_path = os.path.join(max_root_dir, str(i) + '.npy')\n",
    "    paths = [min_full_path, max_full_path]\n",
    "    for path in paths:\n",
    "        x_data_batch, location_data_batch, resolution_data_batch = stage3_load_batch_records(path, fixed_size)\n",
    "        x_data.extend(x_data_batch)\n",
    "        location_data.extend(location_data_batch)\n",
    "        resolution_data.extend(resolution_data_batch)\n",
    "# fixed seperation, for comparing different models\n",
    "n_samples_train = int(len(x_data) * train_val_ratio)\n",
    "n_samples_val = len(x_data) - n_samples_train\n",
    "\n",
    "# split data\n",
    "x_data_train = x_data[:n_samples_train]\n",
    "location_data_train = location_data[:n_samples_train]\n",
    "resolution_data_train = resolution_data[:n_samples_train]\n",
    "volume_data_train = volume_data[:n_samples_train]\n",
    "x_data_val = x_data[n_samples_train:]\n",
    "location_data_val = location_data[n_samples_train:]\n",
    "resolution_data_val = resolution_data[n_samples_train:]\n",
    "volume_data_val = volume_data[n_samples_train:]\n",
    "\n",
    "n_train_batches = n_samples_train\n",
    "n_val_batches = n_samples_val\n",
    "\n",
    "print(\"------------data info---------------------\")\n",
    "print(\"TRAIN, x: {}, location: {}, resolution: {}, volume: {}\".format(len(x_data_train), len(location_data_train),\n",
    "                                                                     len(resolution_data_train), len(volume_data_train)))\n",
    "print(\"VAL, x: {}, location: {}, resolution: {}, volume: {}\".format(len(x_data_val), len(location_data_val),\n",
    "                                                                     len(resolution_data_val), len(volume_data_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x = T.tensor4('x')\n",
    "location = T.vector('location')\n",
    "resolution = T.matrix('resolution')\n",
    "target_volume = T.fscalar('volume')\n",
    "\n",
    "# all adapters\n",
    "adapters = []\n",
    "adapters.append(adapter1((64, 64), '60.npz'))\n",
    "\n",
    "# input tensor\n",
    "pred = T.tensor4('pred')\n",
    "\n",
    "# fusion layers\n",
    "l_in = InputLayer(shape=(None, len(adapters), fixed_size[0], fixed_size[1]), input_var = pred)\n",
    "mid = batch_norm(Conv2DLayer(l_in, num_filters=32, filter_size=(3, 3), nonlinearity=rectify, W=HeNormal()))\n",
    "l_out = GlobalPoolLayer(Conv2DLayer(mid, num_filters=1, filter_size=(1, 1), W=HeNormal()))\n",
    "\n",
    "area = lasagne.layers.get_output(l_out)\n",
    "pred_volume = build_volume(area, location, resolution)\n",
    "loss = T.abs_(pred_volume - target_volume).mean() / 600\n",
    "\n",
    "params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "\n",
    "train_fn = theano.function(\n",
    "    [pred, location, resolution, target_volume],\n",
    "    loss,\n",
    "    updates = updates\n",
    ")\n",
    "\n",
    "val_fn = theano.function(\n",
    "    [pred, location, resolution, target_volume],\n",
    "    loss\n",
    ")\n",
    "\n",
    "test_fn = theano.function(\n",
    "    [pred, location, resolution],\n",
    "    [area, pred_volume]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------snapshot info--------------------\n",
      "snapshot file snapshot/0.npz is not exist\n",
      "cur_iter: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------snapshot info--------------------\")\n",
    "cur_iter = 0\n",
    "if snapshot_root is not None:\n",
    "    if not os.path.exists(snapshot_root):\n",
    "        print(\"creating {}\".format(snapshot_root))\n",
    "        os.mkdir(snapshot_root)\n",
    "    if snapshot_file is not None: \n",
    "        snapshot_full_path = os.path.join(snapshot_root, snapshot_file)\n",
    "        if os.path.exists(snapshot_full_path):\n",
    "            with np.load(snapshot_full_path) as f:\n",
    "                param_values = [f['arr_{}'.format(i)] for i in range(len(f.files))]\n",
    "            print('resuming snapshot from {}'.format(snapshot_full_path))\n",
    "            param_cur = lasagne.layers.get_all_params(l_out)\n",
    "            assert len(param_cur) == len(param_values)\n",
    "            for p, v in zip(param_cur, param_values):\n",
    "                p.set_value(v)\n",
    "            m = re.findall('\\d+', snapshot_file)\n",
    "            if len(m) > 0:\n",
    "                cur_iter = int(m[0])\n",
    "        else:\n",
    "            print(\"snapshot file {} is not exist\".format(snapshot_full_path))\n",
    "print(\"cur_iter: {}\".format(cur_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"-----------train and validation info------------\")\n",
    "if schedules is not None:\n",
    "    for n_epoches in schedules:\n",
    "        print(\"#epochs: {}ï¼Œ lr: {}\".format(n_epoches, lr))\n",
    "        while n_epoches > 0:\n",
    "            cur_iter += 1\n",
    "            residual_epoch = min(n_epoches, 1)\n",
    "            seen_train_samples = int(n_train_batches * residual_epoch)\n",
    "            train_idx = np.random.permutation(n_train_batches)[:seen_train_samples]\n",
    "            val_idx = np.random.permutation(n_val_batches)\n",
    "            train_losses = []\n",
    "            train_accs = []\n",
    "            val_losses = []\n",
    "            val_accs = []\n",
    "            for j in train_idx:\n",
    "                x_e = x_data_train[j].astype('float32')\n",
    "                preds = []\n",
    "                for adapter in adapters:\n",
    "                    preds.append(adapter.convert(x_e))\n",
    "                pred_e = np.concatenate(preds, axis=1)\n",
    "                location_e = location_data_train[j].astype('float32')\n",
    "                resolution_e = resolution_data_train[j].astype('float32')\n",
    "                volume_e = volume_data_train[j].astype('float32')\n",
    "                loss = train_fn(pred_e, location_e, resolution_e, volume_e)\n",
    "                train_losses.append(loss)\n",
    "            for j in val_idx:\n",
    "                x_e = x_data_val[j].astype('float32')\n",
    "                preds = []\n",
    "                for adapter in adapters:\n",
    "                    preds.append(adapter.convert(x_e))\n",
    "                pred_e = np.concatenate(preds, axis=1)\n",
    "                location_e = location_data_val[j].astype('float32')\n",
    "                resolution_e = resolution_data_val[j].astype('float32')\n",
    "                volume_e = volume_data_val[j].astype('float32')\n",
    "                loss = val_fn(pred_e, location_e, resolution_e, volume_e)\n",
    "                val_losses.append(loss)\n",
    "            print(\"stage3: train loss {}[seen {} samples], val loss {}[seen {} samples][{}]\"\\\n",
    "                  .format(np.mean(train_losses), seen_train_samples, \n",
    "                          np.mean(val_losses), n_val_batches, cur_iter))\n",
    "            n_epoches -= 1\n",
    "            if cur_iter % snapshot_iter == 0:\n",
    "                snapshot_save_path = os.path.join(snapshot_root, str(cur_iter) + \".npz\")\n",
    "                print(\"snapshot to {}\".format(snapshot_save_path))\n",
    "                np.savez(snapshot_save_path, *lasagne.layers.get_all_param_values(l_out))\n",
    "else:\n",
    "    print(\"schedules is empty, skip training and validating stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bm.test_on_train_data(test_fn, fixed_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
